# Dockerfile for MLB Data Platform Spark Jobs
# Multi-stage build: base (production) + test (with pytest)

# =============================================================================
# BASE STAGE - Production image with Java 17, Python 3.11, PySpark 3.5
# =============================================================================
FROM eclipse-temurin:17-jre-jammy AS base

# Install Python 3.11 and system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-venv \
    python3-pip \
    curl \
    procps \
    tini \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Upgrade pip and install uv
RUN python3 -m pip install --upgrade pip && \
    python3 -m pip install uv

# Set working directory
WORKDIR /app

# Copy project files (production only)
COPY pyproject.toml uv.lock README.md ./
COPY src/ ./src/
COPY examples/ ./examples/
COPY .git ./.git

# Install Python dependencies using uv
# Set fallback version for setuptools-scm
ENV SETUPTOOLS_SCM_PRETEND_VERSION=0.1.0
RUN uv pip install --system -e .

# Download PostgreSQL JDBC driver
RUN mkdir -p /opt/spark/jars && \
    curl -L https://jdbc.postgresql.org/download/postgresql-42.6.0.jar \
    -o /opt/spark/jars/postgresql-42.6.0.jar

# Set Spark environment variables
ENV SPARK_VERSION=3.5
ENV SPARK_HOME=/usr/local/lib/python3.11/dist-packages/pyspark
ENV PYTHONPATH=/app/src:${PYTHONPATH}

# Pre-download Deequ and Delta JARs to avoid runtime Maven downloads
# This creates the Ivy cache so tests start instantly
RUN python3 -c "\
from pyspark.sql import SparkSession; \
spark = SparkSession.builder \
    .appName('jar-download') \
    .master('local[1]') \
    .config('spark.jars.packages', 'com.amazon.deequ:deequ:2.0.7-spark-3.5,io.delta:delta-spark_2.12:3.3.2') \
    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \
    .getOrCreate(); \
spark.stop(); \
print('JARs downloaded successfully')"

# Download Spark Kubernetes entrypoint scripts (required for Spark Operator)
RUN mkdir -p /opt && \
    curl -L https://raw.githubusercontent.com/apache/spark/v3.5.0/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh \
      -o /opt/entrypoint.sh && \
    curl -L https://raw.githubusercontent.com/apache/spark/v3.5.0/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/decom.sh \
      -o /opt/decom.sh && \
    chmod +x /opt/entrypoint.sh /opt/decom.sh

# Set Spark Kubernetes entrypoint
WORKDIR /opt/spark/work-dir
ENTRYPOINT ["/opt/entrypoint.sh"]


# =============================================================================
# TEST STAGE - Adds pytest and test files for running Spark/PyDeequ tests
# =============================================================================
FROM base AS test

WORKDIR /app

# Copy test files
COPY tests/ ./tests/

# Install test dependencies
RUN uv pip install --system pytest pytest-cov pytest-asyncio

# Override entrypoint for test runs
ENTRYPOINT ["python3", "-m", "pytest"]
CMD ["tests/unit/", "-v"]
