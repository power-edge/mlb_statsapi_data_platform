# Game Transformation Workflow
# Transforms raw game data from PostgreSQL to normalized tables
# Extracts game metadata, team statistics, and player information
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: game-transform
  namespace: mlb-data-platform
  labels:
    app: mlb-data-platform
    component: transform
    endpoint: game
spec:
  entrypoint: game-transform-pipeline

  # Workflow-level parameters
  arguments:
    parameters:
      - name: game-pks
        value: ""  # Comma-separated game PKs, empty = process all
      - name: start-date
        value: ""  # Optional date filter (YYYY-MM-DD)
      - name: end-date
        value: ""  # Optional date filter (YYYY-MM-DD)
      - name: export-to-delta
        value: "false"
      - name: delta-path
        value: "s3://mlb-data/game/live"
      - name: spark-driver-memory
        value: "2g"
      - name: spark-executor-memory
        value: "4g"
      - name: spark-executor-instances
        value: "3"  # Game data can be large

  # Service account for accessing PostgreSQL secrets
  serviceAccountName: mlb-data-platform-sa

  templates:
    # Main pipeline template
    - name: game-transform-pipeline
      steps:
        - - name: validate-inputs
            template: validate-inputs

        - - name: run-spark-transform
            template: spark-transform-job

        - - name: validate-outputs
            template: validate-outputs

    # Input validation
    - name: validate-inputs
      script:
        image: postgres:15-alpine
        command: [sh]
        source: |
          #!/bin/sh
          set -e

          echo "Validating game raw data exists..."

          # Build WHERE clause based on parameters
          WHERE_CLAUSE="1=1"

          if [ -n "{{workflow.parameters.game-pks}}" ]; then
            GAME_PKS_ARRAY=$(echo "{{workflow.parameters.game-pks}}" | tr ',' ' ')
            GAME_PKS_LIST=$(echo $GAME_PKS_ARRAY | sed 's/ /,/g')
            WHERE_CLAUSE="$WHERE_CLAUSE AND game_pk IN ($GAME_PKS_LIST)"
            echo "Filtering by game PKs: $GAME_PKS_LIST"
          fi

          if [ -n "{{workflow.parameters.start-date}}" ]; then
            WHERE_CLAUSE="$WHERE_CLAUSE AND game_date >= '{{workflow.parameters.start-date}}'"
            echo "Start date: {{workflow.parameters.start-date}}"
          fi

          if [ -n "{{workflow.parameters.end-date}}" ]; then
            WHERE_CLAUSE="$WHERE_CLAUSE AND game_date <= '{{workflow.parameters.end-date}}'"
            echo "End date: {{workflow.parameters.end-date}}"
          fi

          COUNT=$(PGPASSWORD=$POSTGRES_PASSWORD psql \
            -h $POSTGRES_HOST \
            -U $POSTGRES_USER \
            -d $POSTGRES_DB \
            -t -c "SELECT COUNT(*) FROM game.live_game_v1 WHERE $WHERE_CLAUSE;")

          echo "Found $COUNT raw game records"

          if [ "$COUNT" -eq "0" ]; then
            echo "ERROR: No raw game data found matching criteria"
            exit 1
          fi

          echo "✓ Validation passed"
        env:
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: postgres-credentials
                key: host
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: postgres-credentials
                key: username
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: postgres-credentials
                key: password
          - name: POSTGRES_DB
            value: mlb_games

    # Spark transformation job
    - name: spark-transform-job
      resource:
        action: create
        successCondition: status.applicationState.state == COMPLETED
        failureCondition: status.applicationState.state == FAILED
        manifest: |
          apiVersion: sparkoperator.k8s.io/v1beta2
          kind: SparkApplication
          metadata:
            name: game-transform-{{workflow.uid}}
            namespace: mlb-data-platform
          spec:
            type: Python
            mode: cluster
            image: mlb-data-platform/spark:latest
            imagePullPolicy: Never
            mainApplicationFile: local:///app/examples/spark_jobs/transform_game.py

            sparkVersion: "3.5.0"
            restartPolicy:
              type: Never

            driver:
              cores: 1
              coreLimit: "1200m"
              memory: "{{workflow.parameters.spark-driver-memory}}"
              labels:
                version: "3.5.0"
                app: mlb-data-platform
              serviceAccount: mlb-data-platform-sa
              env:
                - name: GAME_PKS
                  value: "{{workflow.parameters.game-pks}}"
                - name: START_DATE
                  value: "{{workflow.parameters.start-date}}"
                - name: END_DATE
                  value: "{{workflow.parameters.end-date}}"
                - name: EXPORT_TO_DELTA
                  value: "{{workflow.parameters.export-to-delta}}"
                - name: DELTA_PATH
                  value: "{{workflow.parameters.delta-path}}"
                - name: POSTGRES_HOST
                  valueFrom:
                    secretKeyRef:
                      name: postgres-credentials
                      key: host
                - name: POSTGRES_USER
                  valueFrom:
                    secretKeyRef:
                      name: postgres-credentials
                      key: username
                - name: POSTGRES_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-credentials
                      key: password
                - name: POSTGRES_DB
                  value: mlb_games

            executor:
              cores: 1
              instances: {{workflow.parameters.spark-executor-instances}}
              memory: "{{workflow.parameters.spark-executor-memory}}"
              labels:
                version: "3.5.0"
                app: mlb-data-platform

    # Output validation
    - name: validate-outputs
      script:
        image: postgres:15-alpine
        command: [sh]
        source: |
          #!/bin/sh
          set -e

          echo "Validating transformed game data..."

          # Check metadata table (currently writes to game.live_game_metadata or similar)
          # Note: Adjust table name based on actual implementation
          METADATA_COUNT=$(PGPASSWORD=$POSTGRES_PASSWORD psql \
            -h $POSTGRES_HOST \
            -U $POSTGRES_USER \
            -d $POSTGRES_DB \
            -t -c "SELECT COUNT(*) FROM game.live_game_v1;" || echo "0")

          echo "Found $METADATA_COUNT game metadata records"

          # Check teams table
          TEAMS_COUNT=$(PGPASSWORD=$POSTGRES_PASSWORD psql \
            -h $POSTGRES_HOST \
            -U $POSTGRES_USER \
            -d $POSTGRES_DB \
            -t -c "SELECT COUNT(*) FROM game.live_game_v1;" || echo "0")

          echo "Found $TEAMS_COUNT team records"

          if [ "$METADATA_COUNT" -eq "0" ]; then
            echo "WARNING: No game metadata created"
          fi

          if [ "$TEAMS_COUNT" -eq "0" ]; then
            echo "WARNING: No team records created"
          fi

          echo "✓ Output validation passed"
        env:
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: postgres-credentials
                key: host
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: postgres-credentials
                key: username
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: postgres-credentials
                key: password
          - name: POSTGRES_DB
            value: mlb_games

  # Retry failed steps up to 2 times
  retryStrategy:
    limit: 2
    retryPolicy: "Always"
    backoff:
      duration: "1m"
      factor: 2
      maxDuration: "10m"
